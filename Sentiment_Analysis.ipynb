{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from typing import Dict, List, Set\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class IndonesianTextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stemmer = StemmerFactory().create_stemmer()\n",
        "        self.contractions: Dict[str, str] = self._init_contractions()\n",
        "        self.slang: Dict[str, str] = self._init_slang()\n",
        "        self.stopwords: Set[str] = self._init_stopwords()\n",
        "        self.special_patterns: List[str] = [\n",
        "            r'@\\w+',  # Username mentions\n",
        "            r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+',  # Email addresses\n",
        "            r'https?://\\S+',  # URLs\n",
        "            r'\\#\\w+',  # Hashtags\n",
        "            r'RT[\\s]+',  # Retweets\n",
        "            r'wk+',  # Variasi \"wkwk\"\n",
        "            r'h[ha]+',  # Variasi \"haha\"\n",
        "            r'_+',  # Underscore berulang\n",
        "        ]\n",
        "\n",
        "    def _init_contractions(self) -> Dict[str, str]:\n",
        "        \"\"\"Initialize contraction dictionary with common Indonesian contractions\"\"\"\n",
        "        base_contractions = {\n",
        "            'dgn': 'dengan', 'yg': 'yang', 'utk': 'untuk', 'tdk': 'tidak',\n",
        "            'krn': 'karena', 'hrs': 'harus', 'sdh': 'sudah', 'spy': 'supaya',\n",
        "            'trs': 'terus', 'gk': 'tidak', 'ga': 'tidak', 'nggak': 'tidak',\n",
        "            'gak': 'tidak', 'udh': 'sudah', 'udah': 'sudah', 'klo': 'kalau',\n",
        "            'kalo': 'kalau', 'gitu': 'begitu', 'gmn': 'bagaimana', 'emg': 'memang',\n",
        "            'emang': 'memang', 'bgt': 'banget', 'bngt': 'banget', 'dpat': 'dapat',\n",
        "            'dpt': 'dapat', 'tp': 'tapi', 'tpi': 'tapi', 'skrg': 'sekarang',\n",
        "            'skg': 'sekarang', 'org': 'orang', 'orng': 'orang', 'sy': 'saya',\n",
        "            'sya': 'saya', 'km': 'kamu', 'kmu': 'kamu', 'ak': 'aku', 'aq': 'aku',\n",
        "            'bs': 'bisa', 'bsa': 'bisa', 'br': 'baru', 'bru': 'baru',\n",
        "            'pd': 'pada', 'sprti': 'seperti', 'spt': 'seperti', 'msh': 'masih',\n",
        "            'msi': 'masih', 'dr': 'dari', 'dri': 'dari', 'dlm': 'dalam',\n",
        "            'dal': 'dalam', 'sbg': 'sebagai', 'sbgi': 'sebagai', 'smp': 'sampai',\n",
        "            'smpe': 'sampai', 'cb': 'coba', 'cba': 'coba', 'tdak': 'tidak',\n",
        "            # Tambahan dari dataset baru\n",
        "            'gue': 'saya', 'lu': 'kamu', 'ni': 'ini', 'tu': 'itu',\n",
        "            'kl': 'kalau', 'gt': 'begitu', 'mah': 'memang', 'ya': 'iya',\n",
        "            'yh': 'iya', 'kaya': 'seperti', 'cos': 'karena', 'coz': 'karena',\n",
        "            'gr': 'gara-gara', 'ngga': 'tidak', 'gada': 'tidak ada',\n",
        "            'aj': 'saja', 'aja': 'saja', 'dah': 'sudah', 'udh': 'sudah'\n",
        "        }\n",
        "        return base_contractions\n",
        "\n",
        "    def _init_slang(self) -> Dict[str, str]:\n",
        "        \"\"\"Initialize slang dictionary with common Indonesian informal terms\"\"\"\n",
        "        base_slang = {\n",
        "            'mantap': 'mantap', 'mantul': 'mantap', 'keren': 'bagus',\n",
        "            'kereen': 'bagus', 'kece': 'bagus', 'oke': 'baik', 'ok': 'baik',\n",
        "            'sip': 'baik', 'sippp': 'baik', 'good': 'baik', 'nice': 'baik',\n",
        "            'mantab': 'mantap', 'kenceng': 'cepat', 'lemot': 'lambat',\n",
        "            'lelet': 'lambat', 'telat': 'terlambat', 'telmi': 'lambat mengerti',\n",
        "            'gercep': 'cepat', 'slow': 'lambat', 'fast': 'cepat', 'asik': 'asyik',\n",
        "            'asek': 'asyik', 'asiik': 'asyik', 'asykk': 'asyik',\n",
        "            # Tambahan dari dataset baru\n",
        "            'njirr': 'anjing', 'anjinggg': 'anjing', 'asu': 'anjing',\n",
        "            'bangsat': 'buruk', 'bangsatttt': 'buruk',\n",
        "            'worthit': 'bermanfaat', 'worth': 'bermanfaat',\n",
        "            'php': 'palsu', 'gemes': 'kesal',\n",
        "            'mayan': 'lumayan', 'lumayanlah': 'lumayan',\n",
        "            'bagusnya': 'bagus', 'bagusss': 'bagus',\n",
        "            'seruuu': 'seru', 'seruu': 'seru'\n",
        "        }\n",
        "        return base_slang\n",
        "\n",
        "    def _init_stopwords(self) -> Set[str]:\n",
        "        \"\"\"Initialize comprehensive Indonesian stopwords set\"\"\"\n",
        "        base_stopwords = {\n",
        "            \"yang\", \"di\", \"ke\", \"dari\", \"pada\", \"dalam\", \"untuk\", \"dengan\", \"dan\",\n",
        "            \"akan\", \"tentang\", \"seperti\", \"dapat\", \"juga\", \"sudah\", \"saya\", \"anda\",\n",
        "            \"dia\", \"mereka\", \"kita\", \"ada\", \"tidak\", \"saat\", \"oleh\", \"setelah\",\n",
        "            \"kepada\", \"sebagai\", \"ini\", \"itu\", \"jika\", \"sehingga\", \"karena\",\n",
        "            \"dimana\", \"ketika\", \"yaitu\", \"yakni\", \"daripada\", \"sejak\", \"sambil\",\n",
        "            \"bahwa\", \"namun\", \"menurut\", \"hampir\", \"dimana\", \"bagaimana\", \"selama\",\n",
        "            \"siapa\", \"mengapa\", \"kapan\", \"kemana\", \"apakah\", \"harus\", \"samping\",\n",
        "            \"sedang\", \"selagi\", \"sementara\", \"tetap\", \"apabila\", \"sebelum\",\n",
        "            \"sesudah\", \"supaya\", \"dengan\", \"agar\", \"lain\", \"pula\", \"padahal\",\n",
        "            \"berada\", \"terhadap\", \"semua\", \"belum\", \"atas\", \"bawah\", \"telah\",\n",
        "            \"guna\", \"kali\", \"cara\", \"dalam\", \"tak\", \"per\", \"bagi\", \"serta\",\n",
        "            # Tambahan stopwords khusus konteks Kampus Merdeka\n",
        "            \"kampus\", \"merdeka\", \"kuliah\", \"mahasiswa\", \"program\", \"semester\",\n",
        "            \"belajar\", \"magang\", \"msib\", \"iisma\", \"pmm\", \"pendidikan\",\n",
        "            \"universitas\", \"fakultas\", \"dosen\", \"mata\", \"sks\", \"sistem\",\n",
        "            \"kurikulum\", \"nadiem\"\n",
        "        }\n",
        "        return base_stopwords\n",
        "\n",
        "    def _remove_emoji(self, text: str) -> str:\n",
        "        \"\"\"Remove emoji characters\"\"\"\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "\n",
        "    def remove_special_characters(self, text: str) -> str:\n",
        "        \"\"\"Remove special characters and numbers\"\"\"\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        return text\n",
        "\n",
        "    def remove_multiple_spaces(self, text: str) -> str:\n",
        "        \"\"\"Remove multiple spaces\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def remove_repeated_characters(self, text: str) -> str:\n",
        "        \"\"\"Remove repeated characters\"\"\"\n",
        "        return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "\n",
        "    def expand_contractions(self, text: str) -> str:\n",
        "        \"\"\"Expand contractions using the contractions dictionary\"\"\"\n",
        "        words = text.split()\n",
        "        return ' '.join([self.contractions.get(word.lower(), word) for word in words])\n",
        "\n",
        "    def normalize_slang(self, text: str) -> str:\n",
        "        \"\"\"Normalize slang words using the slang dictionary\"\"\"\n",
        "        words = text.split()\n",
        "        return ' '.join([self.slang.get(word.lower(), word) for word in words])\n",
        "\n",
        "    def preprocess(self, text: str, stem: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Complete preprocessing pipeline for Indonesian text\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text : str\n",
        "            Input text to preprocess\n",
        "        stem : bool, optional (default=True)\n",
        "            Whether to apply stemming\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            Preprocessed text\n",
        "        \"\"\"\n",
        "        # Remove emojis\n",
        "        text = self._remove_emoji(text)\n",
        "\n",
        "        # Remove special patterns\n",
        "        for pattern in self.special_patterns:\n",
        "            text = re.sub(pattern, '', text)\n",
        "\n",
        "        # Case folding\n",
        "        text = text.lower()\n",
        "\n",
        "        # Expand contractions\n",
        "        text = self.expand_contractions(text)\n",
        "\n",
        "        # Normalize slang\n",
        "        text = self.normalize_slang(text)\n",
        "\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "        # Remove special characters and numbers\n",
        "        text = self.remove_special_characters(text)\n",
        "\n",
        "        # Remove repeated characters\n",
        "        text = self.remove_repeated_characters(text)\n",
        "\n",
        "        # Remove multiple spaces\n",
        "        text = self.remove_multiple_spaces(text)\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        tokens = [word for word in tokens if word not in self.stopwords]\n",
        "\n",
        "        # Stemming (optional)\n",
        "        if stem:\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# 1. LOAD DATA\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Jumlah data awal:\", len(df))\n",
        "    return df\n",
        "\n",
        "# 2. PEMROSESAN DATA\n",
        "def prepare_data(df):\n",
        "    # Inisialisasi preprocessor\n",
        "    preprocessor = IndonesianTextPreprocessor()\n",
        "\n",
        "    # Preprocessing menggunakan IndonesianTextPreprocessor\n",
        "    print(\"Melakukan preprocessing...\")\n",
        "    df['cleaned_text'] = df['full_text'].apply(lambda x: preprocessor.preprocess(x, stem=True))\n",
        "\n",
        "    # Vectorization dengan parameter yang dioptimalkan\n",
        "    print(\"Melakukan vectorization...\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        min_df=2,\n",
        "        max_df=0.95,\n",
        "        ngram_range=(1, 2)  # Menggunakan unigram dan bigram\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df['cleaned_text'])\n",
        "    y = df['label']\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, vectorizer\n",
        "\n",
        "def train_evaluate_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'Naive Bayes': BernoulliNB(),\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'SVM': SVC(kernel='linear', random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nMelatih model {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        results[name] = {\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "            'recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "            'f1': f1_score(y_test, y_pred, average='weighted')\n",
        "        }\n",
        "\n",
        "        print(f\"Hasil evaluasi {name}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_results(results):\n",
        "    print(\"\\nPerbandingan Performa Model:\")\n",
        "    comparison_df = pd.DataFrame(results).round(3) * 100\n",
        "    print(comparison_df)\n",
        "\n",
        "    # Menentukan model terbaik berdasarkan F1-score\n",
        "    best_model = max(results.items(), key=lambda x: x[1]['f1'])\n",
        "    print(f\"\\nModel terbaik adalah {best_model[0]} dengan F1-score: {best_model[1]['f1']:.3f}\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    df = load_data('kampus_merdeka_cleaned.csv')\n",
        "\n",
        "    # Prepare data\n",
        "    X_train, X_test, y_train, y_test, vectorizer = prepare_data(df)\n",
        "\n",
        "    # Train and evaluate models\n",
        "    print(\"\\nMelatih dan mengevaluasi model...\")\n",
        "    results = train_evaluate_models(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Analyze results\n",
        "    best_model = analyze_results(results)\n",
        "\n",
        "    print(\"\\nKESIMPULAN:\")\n",
        "    print(f\"1. Model {best_model[0]} menunjukkan performa terbaik dalam klasifikasi sentimen.\")\n",
        "    print(f\"2. Metrik evaluasi model terbaik:\")\n",
        "    for metric, value in best_model[1].items():\n",
        "        print(f\"   - {metric}: {value:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnCHnWsnJ9Zf",
        "outputId": "abc8f2df-4c00-499e-dca8-7689d458b5fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Jumlah data awal: 600\n",
            "Melakukan preprocessing...\n",
            "Melakukan vectorization...\n",
            "\n",
            "Melatih dan mengevaluasi model...\n",
            "\n",
            "Melatih model Random Forest...\n",
            "Hasil evaluasi Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.78      0.75        63\n",
            "           1       0.73      0.67      0.70        57\n",
            "\n",
            "    accuracy                           0.72       120\n",
            "   macro avg       0.73      0.72      0.72       120\n",
            "weighted avg       0.73      0.72      0.72       120\n",
            "\n",
            "\n",
            "Melatih model Naive Bayes...\n",
            "Hasil evaluasi Naive Bayes:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.86      0.78        63\n",
            "           1       0.80      0.63      0.71        57\n",
            "\n",
            "    accuracy                           0.75       120\n",
            "   macro avg       0.76      0.74      0.74       120\n",
            "weighted avg       0.76      0.75      0.75       120\n",
            "\n",
            "\n",
            "Melatih model Logistic Regression...\n",
            "Hasil evaluasi Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.89      0.81        63\n",
            "           1       0.84      0.67      0.75        57\n",
            "\n",
            "    accuracy                           0.78       120\n",
            "   macro avg       0.80      0.78      0.78       120\n",
            "weighted avg       0.79      0.78      0.78       120\n",
            "\n",
            "\n",
            "Melatih model SVM...\n",
            "Hasil evaluasi SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.76        63\n",
            "           1       0.75      0.68      0.72        57\n",
            "\n",
            "    accuracy                           0.74       120\n",
            "   macro avg       0.74      0.74      0.74       120\n",
            "weighted avg       0.74      0.74      0.74       120\n",
            "\n",
            "\n",
            "Perbandingan Performa Model:\n",
            "           Random Forest  Naive Bayes  Logistic Regression   SVM\n",
            "accuracy            72.5         75.0                 78.3  74.2\n",
            "precision           72.5         75.8                 79.3  74.2\n",
            "recall              72.5         75.0                 78.3  74.2\n",
            "f1                  72.4         74.6                 78.0  74.1\n",
            "\n",
            "Model terbaik adalah Logistic Regression dengan F1-score: 0.780\n",
            "\n",
            "KESIMPULAN:\n",
            "1. Model Logistic Regression menunjukkan performa terbaik dalam klasifikasi sentimen.\n",
            "2. Metrik evaluasi model terbaik:\n",
            "   - accuracy: 0.783\n",
            "   - precision: 0.793\n",
            "   - recall: 0.783\n",
            "   - f1: 0.780\n",
            "3. Hasil ini menunjukkan peningkatan performa setelah penyempurnaan preprocessing:\n",
            "   - Penanganan kontraksi kata yang lebih baik\n",
            "   - Normalisasi slang yang lebih komprehensif\n",
            "   - Stemming menggunakan Sastrawi untuk Bahasa Indonesia\n",
            "   - Penggunaan n-gram (unigram dan bigram)\n",
            "   - Stopwords yang lebih komprehensif\n"
          ]
        }
      ]
    }
  ]
}